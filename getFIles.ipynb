{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dauma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for PolyAI/minds14 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/PolyAI/minds14\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e75d9c02be640f396edb2ab06539306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591eeaa5a3f44ef89ab30b1b27a3d29f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c468c3cc0044627824eae9171955a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ddc30d01cb745279adeb8996ab249b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "\n",
    "dataset = load_dataset(\"PolyAI/minds14\", \"en-US\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dauma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9534edb3eef480199a78a8273ebb13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dauma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dauma\\.cache\\huggingface\\hub\\models--facebook--wav2vec2-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\dauma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c92036be57c8486fbedd9b74a10cf509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db48e2b860af4dae906f9e671d48fb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\n",
    "\n",
    "model = AutoModelForAudioClassification.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'C:\\\\Users\\\\dauma\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\ae9c144664ebbd009bda02726c202d641495c42d960710db51c538515f7e5b9c\\\\en-US~JOINT_ACCOUNT\\\\602ba55abb1e6d0fbce92065.wav',\n",
       " 'array': array([ 1.70561689e-05,  2.18727393e-04,  2.28099860e-04, ...,\n",
       "         3.43842403e-05, -5.96366226e-06, -1.76846370e-05]),\n",
       " 'sampling_rate': 16000}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "dataset[0][\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1937f2d83d847ff9ca1f1260aa62b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/563 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=16000,\n",
    "        padding=True,\n",
    "        max_length=100000,\n",
    "        truncation=True,\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_column(\"intent_class\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_values\", \"labels\"])\n",
    "dataloader = DataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0004,  0.0028,  0.0029,  ..., -0.0007,  0.0027,  0.0046])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"input_values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1ecd94955e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([11]), 'input_values': tensor([[ 0.0004,  0.0028,  0.0029,  ..., -0.0007,  0.0027,  0.0046]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dauma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for MLCommons/peoples_speech contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/MLCommons/peoples_speech\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5c635a28424255b3849aca25013bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/4481 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torchaudio\n",
    "\n",
    "# Load a smaller subset of the dataset\n",
    "dataset = load_dataset(\"MLCommons/peoples_speech\", split=\"train\")\n",
    "small_dataset = dataset.shuffle(seed=42).select(range(10000))  # Adjust the range as needed\n",
    "\n",
    "def preprocess_data(batch):\n",
    "    audio = torchaudio.load(batch['file'])[0]\n",
    "    # Normalize and preprocess text and audio as needed\n",
    "    return {'input_text': batch['text'], 'input_audio': audio}\n",
    "\n",
    "processed_dataset = small_dataset.map(preprocess_data)\n",
    "\n",
    "# Load the model and tokenizer\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Fine-tune the model\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_texts = [example['input_text'] for example in batch]\n",
    "    input_audios = [example['input_audio'] for example in batch]\n",
    "    return {'input_texts': input_texts, 'input_audios': input_audios}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation Results: {eval_results}\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./my-tts-model\")\n",
    "tokenizer.save_pretrained(\"./my-tts-model\")\n",
    "\n",
    "# Generate speech from text\n",
    "from transformers import pipeline\n",
    "\n",
    "tts_pipeline = pipeline('text-to-speech', model=model, tokenizer=tokenizer)\n",
    "\n",
    "text = \"Hello, how are you?\"\n",
    "speech = tts_pipeline(text)\n",
    "\n",
    "with open(\"output.wav\", \"wb\") as f:\n",
    "    f.write(speech)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb88ca12b3b4af3b993ddc00a332122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/92.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and extracted: C:\\Users\\dauma\\.cache\\huggingface\\datasets\\downloads\\extracted\\0290af5e2b7bbee14151654d937b992a79a03fce08461dbcc53f438e4fdb5d24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4661d13ac3c84ec5a51f2fb1e301b108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/86.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and extracted: C:\\Users\\dauma\\.cache\\huggingface\\datasets\\downloads\\extracted\\356753ba32cc2a093a63a5b5c4bb46ac59fb2a48f53513b58f5e489b807654e8\n"
     ]
    }
   ],
   "source": [
    "from datasets import DownloadManager\n",
    "\n",
    "# Initialize the DownloadManager\n",
    "dm = DownloadManager()\n",
    "\n",
    "# List of tar file URLs\n",
    "tar_files = [\n",
    "    \"https://huggingface.co/datasets/MLCommons/peoples_speech/resolve/main/train/clean/clean_000000.tar\",\n",
    "    \"https://huggingface.co/datasets/MLCommons/peoples_speech/resolve/main/train/clean/clean_000001.tar\",\n",
    "    # Add all other tar file URLs\n",
    "]\n",
    "\n",
    "# Iterate over the URLs, downloading and extracting each\n",
    "for tar_url in tar_files:\n",
    "    extracted_path = dm.download_and_extract(tar_url)\n",
    "    print(f\"Downloaded and extracted: {extracted_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing details from the first example of the train split\n",
    "first_sample = data['train'][0]\n",
    "print(first_sample['audio'])  # Audio file details\n",
    "print(first_sample['text'])   # Transcription text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6de40b4618474591eaf34d117a7ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_clean000000 = load_dataset(\"json\", data_files=\"clean.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio_document_id', 'identifier', 'text_document_id', 'training_data'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_clean000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\dauma\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\687f6d41c895e3671e86b3088b64780fc39d29f679ebed1704b974f54f227ecc\\\\Digital_Civic_Underground_Presented_by_Julia_Vallera\\\\Digital_Civic_Underground_Presented_by_Julia_Vallera\\\\Digital_Civic_Underground_Presented_by_Julia_Vallera_DOT_flac_00140.flac'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mipd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataset_validation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      5\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Assuming the dataset includes a path to the audio file in the 'audio' field\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dauma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\arrow_dataset.py:2861\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2860\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2861\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dauma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\arrow_dataset.py:2846\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2844\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m   2845\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[1;32m-> 2846\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[0;32m   2848\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2849\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32mc:\\Users\\dauma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\formatting\\formatting.py:633\u001b[0m, in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    631\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[1;32mc:\\Users\\dauma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\formatting\\formatting.py:397\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable, query_type: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n",
      "File \u001b[1;32mc:\\Users\\dauma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\formatting\\formatting.py:438\u001b[0m, in \u001b[0;36mPythonFormatter.format_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyRow(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    437\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_row(pa_table)\n\u001b[1;32m--> 438\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_features_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m row\n",
      "File \u001b[1;32mc:\\Users\\dauma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\formatting\\formatting.py:216\u001b[0m, in \u001b[0;36mPythonFeaturesDecoder.decode_row\u001b[1;34m(self, row)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, row: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01melse\u001b[39;00m row\n",
      "File \u001b[1;32mc:\\Users\\dauma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\features\\features.py:1976\u001b[0m, in \u001b[0;36mFeatures.decode_example\u001b[1;34m(self, example, token_per_repo_id)\u001b[0m\n\u001b[0;32m   1961\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_example\u001b[39m(\u001b[38;5;28mself\u001b[39m, example: \u001b[38;5;28mdict\u001b[39m, token_per_repo_id: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1962\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decode example with custom feature decoding.\u001b[39;00m\n\u001b[0;32m   1963\u001b[0m \n\u001b[0;32m   1964\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1972\u001b[0m \u001b[38;5;124;03m        `dict[str, Any]`\u001b[39;00m\n\u001b[0;32m   1973\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m-> 1976\u001b[0m         column_name: \u001b[43mdecode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1977\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[0;32m   1978\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[0;32m   1979\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m column_name, (feature, value) \u001b[38;5;129;01min\u001b[39;00m zip_dict(\n\u001b[0;32m   1980\u001b[0m             {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m example}, example\n\u001b[0;32m   1981\u001b[0m         )\n\u001b[0;32m   1982\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\dauma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\features\\features.py:1341\u001b[0m, in \u001b[0;36mdecode_nested_example\u001b[1;34m(schema, obj, token_per_repo_id)\u001b[0m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (Audio, Image)):\n\u001b[0;32m   1339\u001b[0m     \u001b[38;5;66;03m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m schema\u001b[38;5;241m.\u001b[39mdecode:\n\u001b[1;32m-> 1341\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32mc:\\Users\\dauma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\features\\audio.py:183\u001b[0m, in \u001b[0;36mAudio.decode_example\u001b[1;34m(self, value, token_per_repo_id)\u001b[0m\n\u001b[0;32m    180\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m DownloadConfig(token\u001b[38;5;241m=\u001b[39mtoken)\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mxopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    184\u001b[0m         array, sampling_rate \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mread(f)\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dauma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\utils\\file_utils.py:1219\u001b[0m, in \u001b[0;36mxopen\u001b[1;34m(file, mode, download_config, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_local_path(main_hop):\n\u001b[0;32m   1217\u001b[0m     \u001b[38;5;66;03m# ignore fsspec-specific kwargs\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmain_hop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;66;03m# add headers and cookies for authentication on the HF Hub and for Google Drive\u001b[39;00m\n\u001b[0;32m   1221\u001b[0m file, storage_options \u001b[38;5;241m=\u001b[39m _prepare_path_and_storage_options(file_str, download_config\u001b[38;5;241m=\u001b[39mdownload_config)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\dauma\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\687f6d41c895e3671e86b3088b64780fc39d29f679ebed1704b974f54f227ecc\\\\Digital_Civic_Underground_Presented_by_Julia_Vallera\\\\Digital_Civic_Underground_Presented_by_Julia_Vallera\\\\Digital_Civic_Underground_Presented_by_Julia_Vallera_DOT_flac_00140.flac'"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import IPython.display as ipd\n",
    "\n",
    "print(dataset_validation[0])\n",
    "idx = 4\n",
    "\n",
    "# Assuming the dataset includes a path to the audio file in the 'audio' field\n",
    "audio_path = dataset_validation[idx]['audio']['path']  # Adjust the key according to the actual data structure\n",
    "print(dataset_validation[idx]['text'])\n",
    "# Load the audio file\n",
    "audio, sr = librosa.load(audio_path, sr=None)  # `sr=None` loads the audio with its original sampling rate\n",
    "\n",
    "# Play the audio\n",
    "ipd.Audio(audio, rate=sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load(SAMPLE_WAV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dauma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for MLCommons/peoples_speech contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/MLCommons/peoples_speech\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_test = load_dataset(\"MLCommons/peoples_speech\", name=\"test\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'audio', 'duration_ms', 'text'],\n",
       "    num_rows: 34898\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 3050 Laptop GPU (UUID: GPU-77e33ac3-9a2c-9ed8-503a-f69960b7bb0e)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: '/content/'\n",
      "c:\\CodeProjects\\University\\3.2_VU\\deep learning\\gmmGroup\\k3s2_gmmGroup\n",
      "Cloning justinjohn0306/TTS-TT2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dauma\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\magics\\osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "fatal: destination path 'TTS-TT2' already exists and is not an empty directory."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] The system cannot find the path specified: '/content/TTS-TT2/'\n",
      "c:\\CodeProjects\\University\\3.2_VU\\deep learning\\gmmGroup\\k3s2_gmmGroup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] The system cannot find the path specified: '/content/TTS-TT2/'\n",
      "c:\\CodeProjects\\University\\3.2_VU\\deep learning\\gmmGroup\\k3s2_gmmGroup\n",
      "Downloading tacotron2 requirements\n",
      "Requirement already satisfied: matplotlib in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: inflect in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (7.2.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: Unidecode in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.3.8)\n",
      "Requirement already satisfied: pillow in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (10.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dauma\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dauma\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from inflect) (10.2.0)\n",
      "Requirement already satisfied: typeguard>=4.0.1 in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from inflect) (4.2.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from inflect) (4.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dauma\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/wkentaro/gdown.git 'C:\\Users\\dauma\\AppData\\Local\\Temp\\pip-req-build-wzxilk4c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/wkentaro/gdown.git\n",
      "  Cloning https://github.com/wkentaro/gdown.git to c:\\users\\dauma\\appdata\\local\\temp\\pip-req-build-wzxilk4c\n",
      "  Resolved https://github.com/wkentaro/gdown.git to commit eeb6995f2e077991576b35956983bbefe34cf057\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gdown==5.2.0) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gdown==5.2.0) (3.13.3)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gdown==5.2.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gdown==5.2.0) (4.66.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4->gdown==5.2.0) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests[socks]->gdown==5.2.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests[socks]->gdown==5.2.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests[socks]->gdown==5.2.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests[socks]->gdown==5.2.0) (2024.2.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests[socks]->gdown==5.2.0) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dauma\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->gdown==5.2.0) (0.4.6)\n",
      "Requirement already satisfied: ffmpeg-normalize in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ffmpeg-normalize) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\dauma\\appdata\\roaming\\python\\python312\\site-packages (from ffmpeg-normalize) (0.4.6)\n",
      "Requirement already satisfied: ffmpeg-progress-yield in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ffmpeg-normalize) (0.7.8)\n",
      "Requirement already satisfied: colorlog in c:\\users\\dauma\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ffmpeg-normalize) (6.8.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tt2 pretrained model using wget\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'chmod' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distributed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m finfo\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply_gradient_allreduce\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistributedSampler\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'distributed'"
     ]
    }
   ],
   "source": [
    "#@markdown ## <font color=\"pink\"> **2. Install Tacotron2 (w/ARPAbet).**  📦\n",
    "%matplotlib inline\n",
    "import os\n",
    "import io\n",
    "%cd /content/\n",
    "if not os.path.isdir(\"/content/TTS-TT2/\"):\n",
    "  print(\"Cloning justinjohn0306/TTS-TT2\")\n",
    "  !git clone https://github.com/justinjohn0306/ARPAtaco2.git TTS-TT2\n",
    "  %cd /content/TTS-TT2/\n",
    "  !git submodule init\n",
    "  !git submodule update\n",
    "%cd /content/TTS-TT2/\n",
    "#NVIDIA's requirements\n",
    "#I believe Colab gives us PyTorch and TF by default so we don't need anything else\n",
    "#Versions specified in requirements.txt have conflicts so that's why we simply get current versions\n",
    "print(\"Downloading tacotron2 requirements\")\n",
    "!pip install matplotlib numpy inflect scipy Unidecode pillow\n",
    "#Our requirements\n",
    "#We'll need gdown to download some really cool things\n",
    "!pip install git+https://github.com/wkentaro/gdown.git\n",
    "import gdown\n",
    "!git submodule init\n",
    "!git submodule update\n",
    "!pip install ffmpeg-normalize\n",
    "!pip install -q unidecode tensorboardX\n",
    "!apt-get -qq install sox\n",
    "!apt-get install pv\n",
    "!apt-get install jq\n",
    "!wget https://raw.githubusercontent.com/tonikelope/megadown/master/megadown -O megadown.sh\n",
    "!chmod 755 megadown.sh\n",
    "#Download NVIDIA's LJSpeech model\n",
    "# Download NVIDIA's LJSpeech model using wget\n",
    "tt2_pretrained_url = \"https://github.com/justinjohn0306/ARPAtaco2/releases/download/pretrained_model/tacotron2_statedict.pt\"\n",
    "tt2_pretrained_path = \"/content/TTS-TT2/tacotron2_statedict.pt\"\n",
    "\n",
    "if not os.path.isfile(tt2_pretrained_path):\n",
    "  print(\"Downloading tt2 pretrained model using wget\")\n",
    "  !wget {tt2_pretrained_url} -O {tt2_pretrained_path}\n",
    "\n",
    "\n",
    "#tt2_pretrained = \"https://drive.google.com/uc?id=1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA\"\n",
    "#if not os.path.isfile(\"/content/TTS-TT2/pretrained_model\"):\n",
    "  #print(\"Downloading tt2 pretrained\")\n",
    "  #gdown.download(tt2_pretrained, \"/content/TTS-TT2/pretrained_model\", quiet=False)\n",
    "\n",
    "latest_downloaded = None\n",
    "\n",
    "import time\n",
    "import logging\n",
    "\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "logging.getLogger('numba').setLevel(logging.WARNING)\n",
    "logging.getLogger('librosa').setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "from numpy import finfo\n",
    "\n",
    "import torch\n",
    "from distributed import apply_gradient_allreduce\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import Tacotron2\n",
    "from data_utils import TextMelLoader, TextMelCollate\n",
    "from loss_function import Tacotron2Loss\n",
    "from logger import Tacotron2Logger\n",
    "from hparams import create_hparams\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import layers\n",
    "from utils import load_wav_to_torch, load_filepaths_and_text\n",
    "from text import text_to_sequence\n",
    "from math import e\n",
    "#from tqdm import tqdm # Terminal\n",
    "#from tqdm import tqdm_notebook as tqdm # Legacy Notebook TQDM\n",
    "from tqdm.notebook import tqdm # Modern Notebook TQDM\n",
    "from distutils.dir_util import copy_tree\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def download_from_google_drive(file_id, file_name):\n",
    "  # download a file from the Google Drive link\n",
    "  !rm -f ./cookie\n",
    "  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id={file_id}\" > /dev/null\n",
    "  confirm_text = !awk '/download/ {print $NF}' ./cookie\n",
    "  confirm_text = confirm_text[0]\n",
    "  !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm={confirm_text}&id={file_id}\" -o {file_name}\n",
    "\n",
    "def create_mels():\n",
    "    print(\"Generating Mels\")\n",
    "    stft = layers.TacotronSTFT(\n",
    "                hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
    "                hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
    "                hparams.mel_fmax)\n",
    "    def save_mel(filename):\n",
    "        audio, sampling_rate = load_wav_to_torch(filename)\n",
    "        if sampling_rate != stft.sampling_rate:\n",
    "            raise ValueError(\"{} {} SR doesn't match target {} SR\".format(filename,\n",
    "                sampling_rate, stft.sampling_rate))\n",
    "        audio_norm = audio / hparams.max_wav_value\n",
    "        audio_norm = audio_norm.unsqueeze(0)\n",
    "        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "        melspec = stft.mel_spectrogram(audio_norm)\n",
    "        melspec = torch.squeeze(melspec, 0).cpu().numpy()\n",
    "        np.save(filename.replace('.wav', ''), melspec)\n",
    "\n",
    "    import glob\n",
    "    wavs = glob.glob('wavs/*.wav')\n",
    "    for i in tqdm(wavs):\n",
    "        save_mel(i)\n",
    "\n",
    "\n",
    "def reduce_tensor(tensor, n_gpus):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
    "    rt /= n_gpus\n",
    "    return rt\n",
    "\n",
    "\n",
    "def init_distributed(hparams, n_gpus, rank, group_name):\n",
    "    assert torch.cuda.is_available(), \"Distributed mode requires CUDA.\"\n",
    "    print(\"Initializing Distributed\")\n",
    "\n",
    "    # Set cuda device so everything is done on the right GPU.\n",
    "    torch.cuda.set_device(rank % torch.cuda.device_count())\n",
    "\n",
    "    # Initialize distributed communication\n",
    "    dist.init_process_group(\n",
    "        backend=hparams.dist_backend, init_method=hparams.dist_url,\n",
    "        world_size=n_gpus, rank=rank, group_name=group_name)\n",
    "\n",
    "    print(\"Done initializing distributed\")\n",
    "\n",
    "\n",
    "def prepare_dataloaders(hparams):\n",
    "    # Get data, data loaders and collate function ready\n",
    "    trainset = TextMelLoader(hparams.training_files, hparams)\n",
    "    valset = TextMelLoader(hparams.validation_files, hparams)\n",
    "    collate_fn = TextMelCollate(hparams.n_frames_per_step)\n",
    "\n",
    "    if hparams.distributed_run:\n",
    "        train_sampler = DistributedSampler(trainset)\n",
    "        shuffle = False\n",
    "    else:\n",
    "        train_sampler = None\n",
    "        shuffle = True\n",
    "\n",
    "    train_loader = DataLoader(trainset, num_workers=1, shuffle=shuffle,\n",
    "                              sampler=train_sampler,\n",
    "                              batch_size=hparams.batch_size, pin_memory=False,\n",
    "                              drop_last=True, collate_fn=collate_fn)\n",
    "    return train_loader, valset, collate_fn\n",
    "\n",
    "\n",
    "def prepare_directories_and_logger(output_directory, log_directory, rank):\n",
    "    if rank == 0:\n",
    "        if not os.path.isdir(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "            os.chmod(output_directory, 0o775)\n",
    "        logger = Tacotron2Logger(os.path.join(output_directory, log_directory))\n",
    "    else:\n",
    "        logger = None\n",
    "    return logger\n",
    "\n",
    "\n",
    "def load_model(hparams):\n",
    "    model = Tacotron2(hparams).cuda()\n",
    "    if hparams.fp16_run:\n",
    "        model.decoder.attention_layer.score_mask_value = finfo('float16').min\n",
    "\n",
    "    if hparams.distributed_run:\n",
    "        model = apply_gradient_allreduce(model)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def warm_start_model(checkpoint_path, model, ignore_layers):\n",
    "    assert os.path.isfile(checkpoint_path)\n",
    "    print(\"Warm starting model from checkpoint '{}'\".format(checkpoint_path))\n",
    "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = checkpoint_dict['state_dict']\n",
    "    if len(ignore_layers) > 0:\n",
    "        model_dict = {k: v for k, v in model_dict.items()\n",
    "                      if k not in ignore_layers}\n",
    "        dummy_dict = model.state_dict()\n",
    "        dummy_dict.update(model_dict)\n",
    "        model_dict = dummy_dict\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    assert os.path.isfile(checkpoint_path)\n",
    "    print(\"Loading checkpoint '{}'\".format(checkpoint_path))\n",
    "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint_dict['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
    "    learning_rate = checkpoint_dict['learning_rate']\n",
    "    iteration = checkpoint_dict['iteration']\n",
    "    print(\"Loaded checkpoint '{}' from iteration {}\" .format(\n",
    "        checkpoint_path, iteration))\n",
    "    return model, optimizer, learning_rate, iteration\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, learning_rate, iteration, filepath):\n",
    "    import random\n",
    "    if True:\n",
    "        print(\"Saving model and optimizer state at iteration {} to {}\".format(\n",
    "            iteration, filepath))\n",
    "        try:\n",
    "            torch.save({'iteration': iteration,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'learning_rate': learning_rate}, filepath)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"interrupt received while saving, waiting for save to complete.\")\n",
    "            torch.save({'iteration': iteration,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(),'learning_rate': learning_rate}, filepath)\n",
    "        print(\"Model Saved\")\n",
    "\n",
    "def plot_alignment(alignment, info=None):\n",
    "    %matplotlib inline\n",
    "    fig, ax = plt.subplots(figsize=(int(alignment_graph_width/100), int(alignment_graph_height/100)))\n",
    "    im = ax.imshow(alignment, cmap='inferno', aspect='auto', origin='lower',\n",
    "                   interpolation='none')\n",
    "    ax.autoscale(enable=True, axis=\"y\", tight=True)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    xlabel = 'Decoder timestep'\n",
    "    if info is not None:\n",
    "        xlabel += '\\n\\n' + info\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Encoder timestep')\n",
    "    plt.tight_layout()\n",
    "    fig.canvas.draw()\n",
    "    plt.show()\n",
    "\n",
    "def validate(model, criterion, valset, iteration, batch_size, n_gpus,\n",
    "             collate_fn, logger, distributed_run, rank, epoch, start_eposh, learning_rate):\n",
    "    \"\"\"Handles all the validation scoring and printing\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_sampler = DistributedSampler(valset) if distributed_run else None\n",
    "        val_loader = DataLoader(valset, sampler=val_sampler, num_workers=1,\n",
    "                                shuffle=False, batch_size=batch_size,\n",
    "                                pin_memory=False, collate_fn=collate_fn)\n",
    "\n",
    "        val_loss = 0.0\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            x, y = model.parse_batch(batch)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            if distributed_run:\n",
    "                reduced_val_loss = reduce_tensor(loss.data, n_gpus).item()\n",
    "            else:\n",
    "                reduced_val_loss = loss.item()\n",
    "            val_loss += reduced_val_loss\n",
    "        val_loss = val_loss / (i + 1)\n",
    "\n",
    "    model.train()\n",
    "    if rank == 0:\n",
    "        print(\"Epoch: {} Validation loss {}: {:9f}  Time: {:.1f}m LR: {:.6f}\".format(epoch, iteration, val_loss,(time.perf_counter()-start_eposh)/60, learning_rate))\n",
    "        logger.log_validation(val_loss, model, y, y_pred, iteration)\n",
    "        if hparams.show_alignments:\n",
    "            %matplotlib inline\n",
    "            _, mel_outputs, gate_outputs, alignments = y_pred\n",
    "            idx = random.randint(0, alignments.size(0) - 1)\n",
    "            plot_alignment(alignments[idx].data.cpu().numpy().T)\n",
    "\n",
    "def train(output_directory, log_directory, checkpoint_path, warm_start, n_gpus,\n",
    "          rank, group_name, hparams, log_directory2, save_interval, backup_interval):\n",
    "    \"\"\"Training and validation logging results to tensorboard and stdout\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    output_directory (string): directory to save checkpoints\n",
    "    log_directory (string) directory to save tensorboard logs\n",
    "    checkpoint_path(string): checkpoint path\n",
    "    n_gpus (int): number of gpus\n",
    "    rank (int): rank of current gpu\n",
    "    hparams (object): comma separated list of \"name=value\" pairs.\n",
    "    \"\"\"\n",
    "    if hparams.distributed_run:\n",
    "        init_distributed(hparams, n_gpus, rank, group_name)\n",
    "\n",
    "    torch.manual_seed(hparams.seed)\n",
    "    torch.cuda.manual_seed(hparams.seed)\n",
    "\n",
    "    model = load_model(hparams)\n",
    "    learning_rate = hparams.learning_rate\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                                 weight_decay=hparams.weight_decay)\n",
    "\n",
    "    if hparams.fp16_run:\n",
    "        from apex import amp\n",
    "        model, optimizer = amp.initialize(\n",
    "            model, optimizer, opt_level='O2')\n",
    "\n",
    "    if hparams.distributed_run:\n",
    "        model = apply_gradient_allreduce(model)\n",
    "\n",
    "    criterion = Tacotron2Loss()\n",
    "\n",
    "    logger = prepare_directories_and_logger(\n",
    "        output_directory, log_directory, rank)\n",
    "\n",
    "    train_loader, valset, collate_fn = prepare_dataloaders(hparams)\n",
    "\n",
    "    # Load checkpoint if one exists\n",
    "    iteration = 0\n",
    "    epoch_offset = 0\n",
    "    if checkpoint_path is not None and os.path.isfile(checkpoint_path):\n",
    "        if warm_start:\n",
    "            model = warm_start_model(\n",
    "                checkpoint_path, model, hparams.ignore_layers)\n",
    "        else:\n",
    "            model, optimizer, _learning_rate, iteration = load_checkpoint(\n",
    "                checkpoint_path, model, optimizer)\n",
    "            if hparams.use_saved_learning_rate:\n",
    "                learning_rate = _learning_rate\n",
    "            iteration += 1  # next iteration is iteration + 1\n",
    "            epoch_offset = max(0, int(iteration / len(train_loader)))\n",
    "    else:\n",
    "      os.path.isfile(\"/content/TTS-TT2/pretrained_model\")\n",
    "      %cd /dev/null\n",
    "      !/content/TTS-TT2/megadown.sh https://mega.nz/#!WXY3RILA!KyoGHtfB_sdhmLFoykG2lKWhh0GFdwMkk7OwAjpQHRo --o pretrained_model\n",
    "      %cd /content/TTS-TT2\n",
    "      model = warm_start_model(\"/content/TTS-TT2/pretrained_model\", model, hparams.ignore_layers)\n",
    "      # download LJSpeech pretrained model if no checkpoint already exists\n",
    "\n",
    "    start_eposh = time.perf_counter()\n",
    "    learning_rate = 0.0\n",
    "    model.train()\n",
    "    is_overflow = False\n",
    "    # ================ MAIN TRAINNIG LOOP! ===================\n",
    "    for epoch in tqdm(range(epoch_offset, hparams.epochs)):\n",
    "        print(\"\\nStarting Epoch: {} Iteration: {}\".format(epoch, iteration))\n",
    "        start_eposh = time.perf_counter() # eposh is russian, not a typo\n",
    "        for i, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            start = time.perf_counter()\n",
    "            if iteration < hparams.decay_start: learning_rate = hparams.A_\n",
    "            else: iteration_adjusted = iteration - hparams.decay_start; learning_rate = (hparams.A_*(e**(-iteration_adjusted/hparams.B_))) + hparams.C_\n",
    "            learning_rate = max(hparams.min_learning_rate, learning_rate) # output the largest number\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = learning_rate\n",
    "\n",
    "            model.zero_grad()\n",
    "            x, y = model.parse_batch(batch)\n",
    "            y_pred = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "            if hparams.distributed_run:\n",
    "                reduced_loss = reduce_tensor(loss.data, n_gpus).item()\n",
    "            else:\n",
    "                reduced_loss = loss.item()\n",
    "            if hparams.fp16_run:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            if hparams.fp16_run:\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    amp.master_params(optimizer), hparams.grad_clip_thresh)\n",
    "                is_overflow = math.isnan(grad_norm)\n",
    "            else:\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), hparams.grad_clip_thresh)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if not is_overflow and rank == 0:\n",
    "                duration = time.perf_counter() - start\n",
    "                logger.log_training(\n",
    "                    reduced_loss, grad_norm, learning_rate, duration, iteration)\n",
    "                #print(\"Batch {} loss {:.6f} Grad Norm {:.6f} Time {:.6f}\".format(iteration, reduced_loss, grad_norm, duration), end='\\r', flush=True)\n",
    "\n",
    "            iteration += 1\n",
    "        validate(model, criterion, valset, iteration,\n",
    "                 hparams.batch_size, n_gpus, collate_fn, logger,\n",
    "                 hparams.distributed_run, rank, epoch, start_eposh, learning_rate)\n",
    "        if (epoch+1) % save_interval == 0 or (epoch+1) == hparams.epochs: # not sure if the latter is necessary\n",
    "            save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path)\n",
    "        if backup_interval > 0 and (epoch+1) % backup_interval == 0:\n",
    "            save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path + \"_epoch_%s\" % (epoch+1))\n",
    "        if log_directory2 != None:\n",
    "            copy_tree(log_directory, log_directory2)\n",
    "def check_dataset(hparams):\n",
    "    from utils import load_wav_to_torch, load_filepaths_and_text\n",
    "    import os\n",
    "    import numpy as np\n",
    "    def check_arr(filelist_arr):\n",
    "        for i, file in enumerate(filelist_arr):\n",
    "            if len(file) > 2:\n",
    "                print(\"|\".join(file), \"\\nhas multiple '|', this may not be an error.\")\n",
    "            if hparams.load_mel_from_disk and '.wav' in file[0]:\n",
    "                print(\"[WARNING]\", file[0], \" in filelist while expecting .npy .\")\n",
    "            else:\n",
    "                if not hparams.load_mel_from_disk and '.npy' in file[0]:\n",
    "                    print(\"[WARNING]\", file[0], \" in filelist while expecting .wav .\")\n",
    "            if (not os.path.exists(file[0])):\n",
    "                print(\"|\".join(file), \"\\n[WARNING] does not exist.\")\n",
    "            if len(file[1]) < 3:\n",
    "                print(\"|\".join(file), \"\\n[info] has no/very little text.\")\n",
    "            if not ((file[1].strip())[-1] in r\"!?,.;:\"):\n",
    "                print(\"|\".join(file), \"\\n[info] has no ending punctuation.\")\n",
    "            mel_length = 1\n",
    "            if hparams.load_mel_from_disk and '.npy' in file[0]:\n",
    "                melspec = torch.from_numpy(np.load(file[0], allow_pickle=True))\n",
    "                mel_length = melspec.shape[1]\n",
    "            if mel_length == 0:\n",
    "                print(\"|\".join(file), \"\\n[WARNING] has 0 duration.\")\n",
    "    print(\"Checking Training Files\")\n",
    "    audiopaths_and_text = load_filepaths_and_text(hparams.training_files) # get split lines from training_files text file.\n",
    "    check_arr(audiopaths_and_text)\n",
    "    print(\"Checking Validation Files\")\n",
    "    audiopaths_and_text = load_filepaths_and_text(hparams.validation_files) # get split lines from validation_files text file.\n",
    "    check_arr(audiopaths_and_text)\n",
    "    print(\"Finished Checking\")\n",
    "\n",
    "warm_start=False#sorry bout that\n",
    "n_gpus=1\n",
    "rank=0\n",
    "group_name=None\n",
    "\n",
    "# ---- DEFAULT PARAMETERS DEFINED HERE ----\n",
    "hparams = create_hparams()\n",
    "model_filename = 'current_model'\n",
    "hparams.training_files = \"filelists/clipper_train_filelist.txt\"\n",
    "hparams.validation_files = \"filelists/clipper_val_filelist.txt\"\n",
    "#hparams.use_mmi=True,          # not used in this notebook\n",
    "#hparams.use_gaf=True,          # not used in this notebook\n",
    "#hparams.max_gaf=0.5,           # not used in this notebook\n",
    "#hparams.drop_frame_rate = 0.2  # not used in this notebook\n",
    "hparams.p_attention_dropout=0.1\n",
    "hparams.p_decoder_dropout=0.1\n",
    "hparams.decay_start = 15000\n",
    "hparams.A_ = 5e-4\n",
    "hparams.B_ = 8000\n",
    "hparams.C_ = 0\n",
    "hparams.min_learning_rate = 1e-5\n",
    "generate_mels = True\n",
    "hparams.show_alignments = True\n",
    "alignment_graph_height = 600\n",
    "alignment_graph_width = 1000\n",
    "hparams.batch_size = 32\n",
    "hparams.load_mel_from_disk = True\n",
    "hparams.ignore_layers = []\n",
    "hparams.epochs = 10000\n",
    "torch.backends.cudnn.enabled = hparams.cudnn_enabled\n",
    "torch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n",
    "output_directory = '/content/drive/My Drive/colab/outdir' # Location to save Checkpoints\n",
    "log_directory = '/content/TTS-TT2/logs' # Location to save Log files locally\n",
    "log_directory2 = '/content/drive/My Drive/colab/logs' # Location to copy log files (done at the end of each epoch to cut down on I/O)\n",
    "checkpoint_path = output_directory+(r'/')+model_filename\n",
    "\n",
    "# ---- Replace .wav with .npy in filelists ----\n",
    "!sed -i -- 's,.wav|,.npy|,g' filelists/*.txt\n",
    "!sed -i -- 's,.wav|,.npy|,g' {hparams.training_files}\n",
    "!sed -i -- 's,.wav|,.npy|,g' {hparams.validation_files}\n",
    "# ---- Replace .wav with .npy in filelists ----\n",
    "\n",
    "%cd /content/TTS-TT2\n",
    "\n",
    "data_path = 'wavs'\n",
    "!mkdir {data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dauma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for MLCommons/peoples_speech contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/MLCommons/peoples_speech\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95249ccade8440aa43afe03888e4a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading metadata file: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00000.flac\n",
      "07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00001.flac\n",
      "07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00002.flac\n",
      "07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00003.flac\n",
      "07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00004.flac\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "# Enable streaming to load the dataset in smaller portions\n",
    "dataset_stream = datasets.load_dataset(\"MLCommons/peoples_speech\", name=\"clean\", split=\"train\", streaming=True)\n",
    "\n",
    "examples_to_load = 5\n",
    "dataset_slice = dataset_stream.take(examples_to_load)\n",
    "# Iterate through the dataset and access data\n",
    "for example in dataset_slice:\n",
    "    print(example['audio']['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Subclasses of Dataset should implement __getitem__.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdataset_stream\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dauma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:60\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T_co:\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubclasses of Dataset should implement __getitem__.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Subclasses of Dataset should implement __getitem__."
     ]
    }
   ],
   "source": [
    "dataset_stream[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "audio_url = 'http://example.com/path/to/audio/file.wav'\n",
    "\n",
    "response = requests.get(audio_url)\n",
    "if response.status_code == 200:\n",
    "    with open('local_audio_file.wav', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(\"File downloaded successfully!\")\n",
    "else:\n",
    "    print(\"Failed to download file.\")\n",
    "\n",
    "# Now you can use this local path to play the file\n",
    "from IPython.display import Audio\n",
    "Audio(filename='local_audio_file.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'TTS'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTTS\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TTS\n\u001b[0;32m      2\u001b[0m tts \u001b[38;5;241m=\u001b[39m TTS(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtts_models/multilingual/multi-dataset/xtts_v2\u001b[39m\u001b[38;5;124m\"\u001b[39m, gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# generate speech by cloning a voice using default settings\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'TTS'"
     ]
    }
   ],
   "source": [
    "from TTS.api import TTS\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)\n",
    "\n",
    "# generate speech by cloning a voice using default settings\n",
    "tts.tts_to_file(text=\"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n",
    "                file_path=\"output.wav\",\n",
    "                speaker_wav=\"/path/to/target/speaker.wav\",\n",
    "                language=\"en\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
