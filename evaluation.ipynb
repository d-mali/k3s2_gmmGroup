{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg is accessible\n",
      "File not found. Please check the file path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|████████████████████████████████████████████████████| 335/335 [03:01<00:00,  1.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# Audio to text\n",
    "\n",
    "import os\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import whisper\n",
    "import subprocess\n",
    "import string\n",
    "\n",
    "#os.environ[\"PATH\"] += os.pathsep + r\"C:\\ffmpeg\\bin\"\n",
    "\n",
    "def check_ffmpeg():\n",
    "    try:\n",
    "        cmd = [\"ffmpeg\", \"-version\"]\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"ffmpeg is accessible\")\n",
    "        else:\n",
    "            print(\"ffmpeg is not accessible\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"ffmpeg not found\")\n",
    "\n",
    "check_ffmpeg()\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "def transcribe_audio(file_path):\n",
    "    audio = whisper.load_audio(file_path)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "    \n",
    "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "    \n",
    "    _, probs = model.detect_language(mel)\n",
    "    \n",
    "    options = whisper.DecodingOptions(fp16=False)\n",
    "    result = whisper.decode(model, mel, options)\n",
    "\n",
    "    transcription = result.text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "\n",
    "    return transcription\n",
    "\n",
    "flac_file_path = r'audioResults/Tacotron2'\n",
    "\n",
    "if os.path.isfile(flac_file_path):\n",
    "    transcription = transcribe_audio(flac_file_path)\n",
    "    print(f\"Final Transcription: {transcription}\")\n",
    "else:\n",
    "    print(\"File not found. Please check the file path.\")\n",
    "\n",
    "\n",
    "wav_directory = r'audioResults/Tacotron2'\n",
    "\n",
    "transcriptions = []\n",
    "for filename in tqdm(os.listdir(wav_directory), desc=\"Processing files\"):\n",
    "    if filename.endswith(\".flac\"):\n",
    "        file_path = os.path.join(wav_directory, filename)\n",
    "        transcription = transcribe_audio(file_path)\n",
    "        transcriptions.append(transcription)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to txt\n",
    "\n",
    "output_file_path = r'audioResults/transcriptions_Tacotron2.txt'\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for transcription in transcriptions:\n",
    "        f.write(transcription + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe9 in position 2381: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m output_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudioResults/transcriptions_VitsModel.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 5\u001b[0m     transcriptions_from_file \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m output_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudioResults/transcriptions_Tacotron2.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe9 in position 2381: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "# txt to List\n",
    "\n",
    "output_file_path = r'audioResults/transcriptions_VitsModel.txt'\n",
    "with open(output_file_path, 'r') as f:\n",
    "    transcriptions_from_file = [line.strip() for line in f]\n",
    "output_file_path = r'audioResults/transcriptions_Tacotron2.txt'\n",
    "with open(output_file_path, 'r') as f:\n",
    "    transcriptions_from_file2 = [line.strip() for line in f]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"clean.json\")\n",
    "\n",
    "textInput = [] \n",
    "audioPathInput = [] \n",
    "\n",
    "for data in dataset['train']['training_data']:\n",
    "    textInput.extend(data['label'])\n",
    "    audioPathInput.extend(data['name'])\n",
    "\n",
    "printInputArrays = False\n",
    "\n",
    "if printInputArrays:\n",
    "    for i in range(len(textInput)):\n",
    "        print(textInput[i])\n",
    "\n",
    "    for i in range(len(audioPathInput)):\n",
    "        print(audioPathInput[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average WER Model 1: 1.2331827687700017\n",
      "Average WER Model 2: 1.2331827687700017\n"
     ]
    }
   ],
   "source": [
    "# Calculate the WER between two texts\n",
    "\n",
    "# Objective Evaluation\n",
    "# Word Error Rate (WER):\n",
    "\n",
    "# Transcribe the generated audio back to text using an automatic speech recognition (ASR) system.\n",
    "# Compare the transcribed text with the original text to calculate the WER. The model with the lower WER is better.\n",
    "\n",
    "from jiwer import wer\n",
    "\n",
    "if len(textInput)==len(transcriptions_from_file):\n",
    "    original_texts = textInput\n",
    "    transcribed_text_model1 = transcriptions_from_file\n",
    "    transcribed_text_model2 = transcriptions_from_file2\n",
    "\n",
    "    wer_list_model1 = [wer(original, transcribed) for original, transcribed in zip(original_texts, transcribed_text_model1)] if transcribed_text_model1 else []\n",
    "    wer_list_model2 = [wer(original, transcribed) for original, transcribed in zip(original_texts, transcribed_text_model2)] if transcribed_text_model2 else []\n",
    "\n",
    "    average_wer_model1 = sum(wer_list_model1) / len(wer_list_model1) if wer_list_model1 else float('inf')\n",
    "    average_wer_model2 = sum(wer_list_model2) / len(wer_list_model2) if wer_list_model2 else float('inf')\n",
    "\n",
    "    print(f\"Average WER Model 1: {average_wer_model1}\")\n",
    "    print(f\"Average WER Model 2: {average_wer_model2}\")\n",
    "else:\n",
    "    print(\"Input arrays are not of the same length.\")\n",
    "    print(f\"textInput: {len(textInput)} transcriptions_from_file: {len(transcriptions_from_file)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptual Evaluation of Speech Quality (PESQ):\n",
    "\n",
    "# Use PESQ to measure the quality of the speech signals. PESQ compares the generated audio to a reference audio signal and gives a quality score.\n",
    "# Libraries like pypesq can be used for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "CouldntDecodeError",
     "evalue": "Decoding failed. ffmpeg returned error code: 183\n\nOutput from ffmpeg/avlib:\n\nffmpeg version n6.1.1 Copyright (c) 2000-2023 the FFmpeg developers\n  built with gcc 13.2.1 (GCC) 20230801\n  configuration: --prefix=/usr --disable-debug --disable-static --disable-stripping --enable-amf --enable-avisynth --enable-cuda-llvm --enable-lto --enable-fontconfig --enable-frei0r --enable-gmp --enable-gnutls --enable-gpl --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libdav1d --enable-libdrm --enable-libfreetype --enable-libfribidi --enable-libgsm --enable-libharfbuzz --enable-libiec61883 --enable-libjack --enable-libjxl --enable-libmodplug --enable-libmp3lame --enable-libopencore_amrnb --enable-libopencore_amrwb --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libplacebo --enable-libpulse --enable-librav1e --enable-librsvg --enable-librubberband --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtheora --enable-libv4l2 --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpl --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxcb --enable-libxml2 --enable-libxvid --enable-libzimg --enable-nvdec --enable-nvenc --enable-opencl --enable-opengl --enable-shared --enable-vapoursynth --enable-version3 --enable-vulkan\n  libavutil      58. 29.100 / 58. 29.100\n  libavcodec     60. 31.102 / 60. 31.102\n  libavformat    60. 16.100 / 60. 16.100\n  libavdevice    60.  3.100 / 60.  3.100\n  libavfilter     9. 12.100 /  9. 12.100\n  libswscale      7.  5.100 /  7.  5.100\n  libswresample   4. 12.100 /  4. 12.100\n  libpostproc    57.  3.100 / 57.  3.100\n[flac @ 0x5ab7436c3600] Could not find codec parameters for stream 0 (Audio: flac, 0 channels): unspecified sample format\nConsider increasing the value for the 'analyzeduration' (0) and 'probesize' (5000000) options\nInput #0, flac, from 'audioResults/VitsModel/07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00000.flac':\n  Duration: N/A, bitrate: N/A\n  Stream #0:0: Audio: flac, 0 channels\nStream mapping:\n  Stream #0:0 -> #0:0 (flac (native) -> pcm_s32le (native))\nPress [q] to stop, [?] for help\nCannot determine format of input stream 0:0 after EOF\nError marking filters as finished\nError while filtering: Invalid data found when processing input\n[out#0/wav @ 0x5ab7436e7240] Nothing was written into output file, because at least one of its streams received no packets.\nsize=       0kB time=N/A bitrate=N/A speed=N/A    \nConversion failed!\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCouldntDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 21\u001b[0m\n\u001b[1;32m     16\u001b[0m ref_wav_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.wav\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#gen_wav_path_model1 = r'C:\\CodeProjects\\University\\3.2_VU\\deep learning\\gmmGroup\\audioResults\\VitsModel\\07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00000.wav'\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#gen_wav_path_model2 = r'C:\\CodeProjects\\University\\3.2_VU\\deep learning\\gmmGroup\\audioResults\\VitsModel\\07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00001.wav'\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mconvert_flac_to_wav\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_flac_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_wav_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#convert_flac_to_wav(gen_flac_path_model1, gen_wav_path_model1)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#convert_flac_to_wav(gen_flac_path_model2, gen_wav_path_model2)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Read WAV files\u001b[39;00m\n\u001b[1;32m     26\u001b[0m rate, ref_audio \u001b[38;5;241m=\u001b[39m wavfile\u001b[38;5;241m.\u001b[39mread(ref_wav_path)\n",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m, in \u001b[0;36mconvert_flac_to_wav\u001b[0;34m(flac_file_path, wav_file_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_flac_to_wav\u001b[39m(flac_file_path, wav_file_path):\n\u001b[0;32m----> 6\u001b[0m     audio \u001b[38;5;241m=\u001b[39m \u001b[43mAudioSegment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflac_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflac\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     audio\u001b[38;5;241m.\u001b[39mexport(wav_file_path, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.12/site-packages/pydub/audio_segment.py:773\u001b[0m, in \u001b[0;36mAudioSegment.from_file\u001b[0;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m close_file:\n\u001b[1;32m    772\u001b[0m         file\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 773\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CouldntDecodeError(\n\u001b[1;32m    774\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecoding failed. ffmpeg returned error code: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOutput from ffmpeg/avlib:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    775\u001b[0m             p\u001b[38;5;241m.\u001b[39mreturncode, p_err\u001b[38;5;241m.\u001b[39mdecode(errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m) ))\n\u001b[1;32m    777\u001b[0m p_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(p_out)\n\u001b[1;32m    778\u001b[0m fix_wav_headers(p_out)\n",
      "\u001b[0;31mCouldntDecodeError\u001b[0m: Decoding failed. ffmpeg returned error code: 183\n\nOutput from ffmpeg/avlib:\n\nffmpeg version n6.1.1 Copyright (c) 2000-2023 the FFmpeg developers\n  built with gcc 13.2.1 (GCC) 20230801\n  configuration: --prefix=/usr --disable-debug --disable-static --disable-stripping --enable-amf --enable-avisynth --enable-cuda-llvm --enable-lto --enable-fontconfig --enable-frei0r --enable-gmp --enable-gnutls --enable-gpl --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libdav1d --enable-libdrm --enable-libfreetype --enable-libfribidi --enable-libgsm --enable-libharfbuzz --enable-libiec61883 --enable-libjack --enable-libjxl --enable-libmodplug --enable-libmp3lame --enable-libopencore_amrnb --enable-libopencore_amrwb --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libplacebo --enable-libpulse --enable-librav1e --enable-librsvg --enable-librubberband --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtheora --enable-libv4l2 --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpl --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxcb --enable-libxml2 --enable-libxvid --enable-libzimg --enable-nvdec --enable-nvenc --enable-opencl --enable-opengl --enable-shared --enable-vapoursynth --enable-version3 --enable-vulkan\n  libavutil      58. 29.100 / 58. 29.100\n  libavcodec     60. 31.102 / 60. 31.102\n  libavformat    60. 16.100 / 60. 16.100\n  libavdevice    60.  3.100 / 60.  3.100\n  libavfilter     9. 12.100 /  9. 12.100\n  libswscale      7.  5.100 /  7.  5.100\n  libswresample   4. 12.100 /  4. 12.100\n  libpostproc    57.  3.100 / 57.  3.100\n[flac @ 0x5ab7436c3600] Could not find codec parameters for stream 0 (Audio: flac, 0 channels): unspecified sample format\nConsider increasing the value for the 'analyzeduration' (0) and 'probesize' (5000000) options\nInput #0, flac, from 'audioResults/VitsModel/07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00000.flac':\n  Duration: N/A, bitrate: N/A\n  Stream #0:0: Audio: flac, 0 channels\nStream mapping:\n  Stream #0:0 -> #0:0 (flac (native) -> pcm_s32le (native))\nPress [q] to stop, [?] for help\nCannot determine format of input stream 0:0 after EOF\nError marking filters as finished\nError while filtering: Invalid data found when processing input\n[out#0/wav @ 0x5ab7436e7240] Nothing was written into output file, because at least one of its streams received no packets.\nsize=       0kB time=N/A bitrate=N/A speed=N/A    \nConversion failed!\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "from scipy.io import wavfile\n",
    "import os\n",
    "\n",
    "def convert_flac_to_wav(flac_file_path, wav_file_path):\n",
    "    audio = AudioSegment.from_file(flac_file_path, format=\"flac\")\n",
    "    audio.export(wav_file_path, format=\"wav\")\n",
    "\n",
    "# File paths\n",
    "ref_flac_path = r'audioResults/VitsModel/07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00000.flac'\n",
    "#gen_flac_path_model1 = r'C:\\CodeProjects\\University\\3.2_VU\\deep learning\\gmmGroup\\audioResults\\VitsModel\\07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00000.flac'\n",
    "#gen_flac_path_model2 = r'C:\\CodeProjects\\University\\3.2_VU\\deep learning\\gmmGroup\\audioResults\\VitsModel\\07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00001.flac'\n",
    "\n",
    "\n",
    "# File paths\n",
    "ref_wav_path = r'output.wav'\n",
    "#gen_wav_path_model1 = r'C:\\CodeProjects\\University\\3.2_VU\\deep learning\\gmmGroup\\audioResults\\VitsModel\\07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00000.wav'\n",
    "#gen_wav_path_model2 = r'C:\\CodeProjects\\University\\3.2_VU\\deep learning\\gmmGroup\\audioResults\\VitsModel\\07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00001.wav'\n",
    "\n",
    "\n",
    "convert_flac_to_wav(ref_flac_path, ref_wav_path)\n",
    "#convert_flac_to_wav(gen_flac_path_model1, gen_wav_path_model1)\n",
    "#convert_flac_to_wav(gen_flac_path_model2, gen_wav_path_model2)\n",
    "\n",
    "# Read WAV files\n",
    "rate, ref_audio = wavfile.read(ref_wav_path)\n",
    "#rate, gen_audio_model1 = wavfile.read(gen_wav_path_model1)\n",
    "#rate, gen_audio_model2 = wavfile.read(gen_wav_path_model2)\n",
    "\n",
    "# Compute PESQ scores\n",
    "pesq_score_model1 = pesq(rate, ref_audio, gen_audio_model1, 'wb')\n",
    "#pesq_score_model2 = pesq(rate, ref_audio, gen_audio_model2, 'wb')\n",
    "\n",
    "print(f\"PESQ Score Model 1: {pesq_score_model1}\")\n",
    "#print(f\"PESQ Score Model 2: {pesq_score_model2}\")\n",
    "\n",
    "# Clean up temporary WAV files if needed\n",
    "os.remove(ref_wav_path)\n",
    "os.remove(gen_wav_path_model1)\n",
    "os.remove(gen_wav_path_model2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pypesq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpypesq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pesq\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wavfile\n\u001b[1;32m      4\u001b[0m rate, ref_audio \u001b[38;5;241m=\u001b[39m wavfile\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCodeProjects\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUniversity\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m3.2_VU\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdeep learning\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mgmmGroup\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00000.flac\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pypesq'"
     ]
    }
   ],
   "source": [
    "from pypesq import pesq\n",
    "from scipy.io import wavfile\n",
    "\n",
    "rate, ref_audio = wavfile.read(r'C:\\CodeProjects\\University\\3.2_VU\\deep learning\\gmmGroup\\data\\07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00000.flac')\n",
    "rate, gen_audio_model1 = wavfile.read(r'C:\\CodeProjects\\University\\3.2_VU\\deep learning\\gmmGroup\\audioResults\\VitsModel\\07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00000.flac')\n",
    "rate, gen_audio_model2 = wavfile.read(r'C:\\CodeProjects\\University\\3.2_VU\\deep learning\\gmmGroup\\audioResults\\VitsModel\\07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00001.flac')\n",
    "\n",
    "pesq_score_model1 = pesq(rate, ref_audio, gen_audio_model1, 'wb')\n",
    "pesq_score_model2 = pesq(rate, ref_audio, gen_audio_model2, 'wb')\n",
    "\n",
    "print(f\"PESQ Score Model 1: {pesq_score_model1}\")\n",
    "print(f\"PESQ Score Model 2: {pesq_score_model2}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
