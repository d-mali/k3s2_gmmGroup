{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0aeb4a-0044-476f-857b-025422a264f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy scipy librosa unidecode inflect librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a721023-ac62-4176-b47b-aef7d05f89de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dauma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\hub.py:294: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/NVIDIA/DeepLearningExamples/zipball/torchhub\" to C:\\Users\\dauma/.cache\\torch\\hub\\torchhub.zip\n",
      "C:\\Users\\dauma/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\\PyTorch\\Classification\\ConvNets\\image_classification\\models\\common.py:13: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n",
      "C:\\Users\\dauma/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\\PyTorch\\Classification\\ConvNets\\image_classification\\models\\efficientnet.py:17: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n",
      "Downloading checkpoint from https://api.ngc.nvidia.com/v2/models/nvidia/tacotron2_pyt_ckpt_amp/versions/19.09.0/files/nvidia_tacotron2pyt_fp16_20190427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tacotron2(\n",
       "  (embedding): Embedding(148, 512)\n",
       "  (encoder): Encoder(\n",
       "    (convolutions): ModuleList(\n",
       "      (0-2): 3 x Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (lstm): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (prenet): Prenet(\n",
       "      (layers): ModuleList(\n",
       "        (0): LinearNorm(\n",
       "          (linear_layer): Linear(in_features=80, out_features=256, bias=False)\n",
       "        )\n",
       "        (1): LinearNorm(\n",
       "          (linear_layer): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (attention_rnn): LSTMCell(768, 1024)\n",
       "    (attention_layer): Attention(\n",
       "      (query_layer): LinearNorm(\n",
       "        (linear_layer): Linear(in_features=1024, out_features=128, bias=False)\n",
       "      )\n",
       "      (memory_layer): LinearNorm(\n",
       "        (linear_layer): Linear(in_features=512, out_features=128, bias=False)\n",
       "      )\n",
       "      (v): LinearNorm(\n",
       "        (linear_layer): Linear(in_features=128, out_features=1, bias=False)\n",
       "      )\n",
       "      (location_layer): LocationLayer(\n",
       "        (location_conv): ConvNorm(\n",
       "          (conv): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\n",
       "        )\n",
       "        (location_dense): LinearNorm(\n",
       "          (linear_layer): Linear(in_features=32, out_features=128, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder_rnn): LSTMCell(1536, 1024, bias=1)\n",
       "    (linear_projection): LinearNorm(\n",
       "      (linear_layer): Linear(in_features=1536, out_features=80, bias=True)\n",
       "    )\n",
       "    (gate_layer): LinearNorm(\n",
       "      (linear_layer): Linear(in_features=1536, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (postnet): Postnet(\n",
       "    (convolutions): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1-3): 3 x Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): ConvNorm(\n",
       "          (conv): Conv1d(512, 80, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')\n",
    "tacotron2 = tacotron2.to('cuda')\n",
    "tacotron2.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "881cc631-355a-4a70-97b3-534cfb0d586d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\dauma/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\n",
      "Downloading checkpoint from https://api.ngc.nvidia.com/v2/models/nvidia/waveglow_ckpt_amp/versions/19.09.0/files/nvidia_waveglowpyt_fp16_20190427\n",
      "c:\\Users\\dauma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WaveGlow(\n",
       "  (upsample): ConvTranspose1d(80, 80, kernel_size=(1024,), stride=(256,))\n",
       "  (WN): ModuleList(\n",
       "    (0-3): 4 x WN(\n",
       "      (in_layers): ModuleList(\n",
       "        (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "        (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "        (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "        (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
       "        (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
       "        (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
       "        (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
       "      )\n",
       "      (res_skip_layers): ModuleList(\n",
       "        (0-6): 7 x Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
       "        (7): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (cond_layers): ModuleList(\n",
       "        (0-7): 8 x Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (start): Conv1d(4, 512, kernel_size=(1,), stride=(1,))\n",
       "      (end): Conv1d(512, 8, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (4-7): 4 x WN(\n",
       "      (in_layers): ModuleList(\n",
       "        (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "        (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "        (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "        (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
       "        (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
       "        (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
       "        (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
       "      )\n",
       "      (res_skip_layers): ModuleList(\n",
       "        (0-6): 7 x Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
       "        (7): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (cond_layers): ModuleList(\n",
       "        (0-7): 8 x Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (start): Conv1d(3, 512, kernel_size=(1,), stride=(1,))\n",
       "      (end): Conv1d(512, 6, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (8-11): 4 x WN(\n",
       "      (in_layers): ModuleList(\n",
       "        (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))\n",
       "        (2): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))\n",
       "        (3): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,))\n",
       "        (4): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,))\n",
       "        (5): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,))\n",
       "        (6): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,))\n",
       "        (7): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,))\n",
       "      )\n",
       "      (res_skip_layers): ModuleList(\n",
       "        (0-6): 7 x Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
       "        (7): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (cond_layers): ModuleList(\n",
       "        (0-7): 8 x Conv1d(640, 1024, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (start): Conv1d(2, 512, kernel_size=(1,), stride=(1,))\n",
       "      (end): Conv1d(512, 4, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (convinv): ModuleList(\n",
       "    (0-3): 4 x Invertible1x1Conv(\n",
       "      (conv): Conv1d(8, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    )\n",
       "    (4-7): 4 x Invertible1x1Conv(\n",
       "      (conv): Conv1d(6, 6, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    )\n",
       "    (8-11): 4 x Invertible1x1Conv(\n",
       "      (conv): Conv1d(4, 4, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp16')\n",
    "waveglow = waveglow.remove_weightnorm(waveglow)\n",
    "waveglow = waveglow.to('cuda')\n",
    "waveglow.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95881c64-b82d-4eb2-8fb6-ed26e3946d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello world, I missed you so much.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2477a13-f0cb-415e-9ef8-6d1118a8c41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    }
   ],
   "source": [
    "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')\n",
    "sequences, lengths = utils.prepare_input_sequence([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb66476a-98da-48bc-8f0d-479ecab5d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    mel, _, _ = tacotron2.infer(sequences, lengths)\n",
    "    audio = waveglow.infer(mel)\n",
    "audio_numpy = audio[0].data.cpu().numpy()\n",
    "rate = 22050\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f84fbf7d-46de-487b-b4c8-b2f71702beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import write\n",
    "write(\"audio.wav\", rate, audio_numpy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
